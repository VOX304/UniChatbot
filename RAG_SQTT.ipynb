{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VOX304/SchoolChatbot/blob/main/RAG_SQTT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X15KmGsrA-b9"
      },
      "source": [
        "#Packages setting up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T57Y2pVRXf6E",
        "outputId": "025e63fc-b20a-4414-e734-3f56860f82ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.20)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.11/dist-packages (0.3.44)\n",
            "Requirement already satisfied: langchain_google_genai in /usr/local/lib/python3.11/dist-packages (2.1.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.13)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.39)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.8.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (4.12.2)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.16 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (0.6.17)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (2.24.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (4.25.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.69.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.62.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "%pip install langchain \\\n",
        "langchain_community \\\n",
        "langchain_core \\\n",
        "langchain_google_genai \\\n",
        "python-dotenv \\\n",
        "pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLaao_cDlUls",
        "outputId": "c319a003-8133-4d29-90e4-e22fd4e82c0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2ZZGF2_Bc-z",
        "outputId": "5da2fce2-4885-480f-b1d8-a8e3564d8e39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install scikit-learn \\\n",
        "numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTx2gGAeA508",
        "outputId": "ac3d643d-6764-4a18-af2d-c93dc994a30b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.48.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.28.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "cmFEYpAKhjHt"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "VietTransformer = SentenceTransformer('dangvantuan/vietnamese-document-embedding', trust_remote_code=True)\n",
        "#embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
        "pdf_files = ['/content/sample_data/KinhTevPhatTrien2024.pdf',\n",
        "             '/content/sample_data/KinhTevPhatTrien2025.pdf',\n",
        "             '/content/sample_data/KinhTevPhatTrien2025_t2.pdf']  # Adjust paths"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pdf_files)"
      ],
      "metadata": {
        "id": "SUh_b_f6VYIN",
        "outputId": "1fcb09d1-4646-497f-f7ab-7780b5cd2771",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/sample_data/KinhTevPhatTrien2024.pdf', '/content/sample_data/KinhTevPhatTrien2025.pdf', '/content/sample_data/KinhTevPhatTrien2025_t2.pdf']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfRZiKG73qBP"
      },
      "source": [
        "#PDF-Preprocessing & VectorDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9FNPA0Imh3TV",
        "outputId": "dd3acd06-11bf-4e5a-b15d-2c5af6261719"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ Extracted content saved to extracted_content.txt\n"
          ]
        }
      ],
      "source": [
        "documents = []\n",
        "for pdf in pdf_files:\n",
        "    pdf_loader = PyPDFLoader(pdf)\n",
        "    documents.extend(pdf_loader.load())\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "# Ensure embeddings are generated correctly\n",
        "#embeddings = embedding_model.embed_documents([doc.page_content for doc in chunks])\n",
        "\n",
        "# Pass embedded vectors to FAISS\n",
        "\n",
        "with open(\"extracted_content.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        f.write(f\"Chunk {i+1}:\\n{chunk.page_content}\\n\\n{'='*50}\\n\\n\")\n",
        "\n",
        "print(\"üìÑ Extracted content saved to extracted_content.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "class CustomEmbedding:\n",
        "    def __init__(self, model_name):\n",
        "        self.model = SentenceTransformer(model_name, trust_remote_code=True)  # ‚úÖ Add trust_remote_code=True\n",
        "\n",
        "    def embed_documents(self, texts):\n",
        "        return self.model.encode(texts, convert_to_numpy=True)\n",
        "\n",
        "    def embed_query(self, text):\n",
        "        return self.model.encode([text], convert_to_numpy=True)[0]\n",
        "    def __call__(self, text):\n",
        "        \"\"\"Make the class callable, so FAISS can use it.\"\"\"\n",
        "        return self.embed_query(text)\n",
        "\n",
        "# Initialize the embedding model\n",
        "embedding_model = CustomEmbedding(\"dangvantuan/vietnamese-document-embedding\")\n",
        "\n",
        "# Generate embeddings\n",
        "doc_embeddings = embedding_model.embed_documents([\"Xin ch√†o!\", \"H√† N·ªôi l√† th·ªß ƒë√¥ c·ªßa Vi·ªát Nam.\"])\n",
        "query_embedding = embedding_model.embed_query(\"Th√†nh ph·ªë n√†o l√† th·ªß ƒë√¥?\")\n",
        "\n",
        "print(doc_embeddings.shape, query_embedding.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdIcGjKnAtk1",
        "outputId": "245c5ba8-fc79-4e1a-efe9-3d4373442c2e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 768) (768,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Create the FAISS vector store\n",
        "vector_db = FAISS.from_documents(chunks, embedding_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKX0S0mz_e4w",
        "outputId": "b2231a2a-49eb-44be-cd2d-1b002e65c799"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "z_Zy1uOzh0Yw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f17ba40-bc8d-477e-da80-1683560c0800"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Processed 3010 text chunks into FAISS vector database.\n"
          ]
        }
      ],
      "source": [
        "print(f\"‚úÖ Processed {len(chunks)} text chunks into FAISS vector database.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "S6DA2G3NmVFl"
      },
      "outputs": [],
      "source": [
        "query = \"ƒë·ªô tu·ªïi GenZ\"\n",
        "retrieved_docs = vector_db.similarity_search(query, k = 4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "7AL0_aPymcvs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bbc6a0a-48ed-49c8-ccd6-3be1171c3d7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìÑ Document 1:\n",
            "S·ªë 330 th√°ng 12/2024 66\n",
            "1. Gi·ªõi thi·ªáu\n",
            "Th·∫ø h·ªá Z th∆∞·ªùng ƒë∆∞·ª£c c√°c nh√† nghi√™n c·ª©u x√°c ƒë·ªãnh l√† sinh ra trong kho·∫£ng th·ªùi gian t·ª´ 1995 ‚Äì 2012 \n",
            "(Barhate & Dirani, 2022; Maloni & c·ªông s·ª±, 2019). V·ªõi vi·ªác ƒë∆∞·ª£c ƒë√†o t·∫°o tr√¨nh ƒë·ªô ƒë·∫°i h·ªçc, nh√≥m n√†y ƒë√£ \n",
            "gia nh·∫≠p th·ªã tr∆∞·ªùng lao ƒë·ªông ƒë∆∞·ª£c kho·∫£ng 7 nƒÉm v√† ƒëang d·∫ßn tr·ªü th√†nh l·ª±c l∆∞·ª£ng lao ƒë·ªông ch√≠nh, ƒë·∫∑c bi·ªát l√† \n",
            "trong lƒ©nh v·ª±c kinh doanh v√† qu·∫£n l√Ω. Trong b·ªëi c·∫£nh chuy·ªÉn ƒë·ªïi l·ª±c l∆∞·ª£ng lao ƒë·ªông nh∆∞ v·∫≠y, r·∫•t c·∫ßn thi·∫øt\n",
            "\n",
            "üìÑ Document 2:\n",
            "2.2. Th·∫ø h·ªá Z\n",
            "L√† th·∫ø h·ªá m·ªõi nh·∫•t tham gia v√†o l·ª±c l∆∞·ª£ng lao ƒë·ªông, th·∫ø h·ªá Z cho th·∫•y s·ª± kh√°c bi·ªát ƒë√°ng k·ªÉ trong h√†nh \n",
            "vi v√† th√°i ƒë·ªô ƒë·ªëi v·ªõi c√¥ng vi·ªác so v·ªõi nh·ªØng th·∫ø h·ªá tr∆∞·ªõc ƒë√¢y. Vi·ªác c√≥ th·ªùi gian ƒëi h·ªçc d√†i, c√≥ s·ª± bao tr√πm \n",
            "c·ªßa c√¥ng ngh·ªá v√† thi·∫øt b·ªã di ƒë·ªông, ƒë∆∞·ª£c s·ªëng trong m·ªôt x√£ h·ªôi ph√°t tri·ªÉn h∆°n ƒë√£ t·∫°o ra m·ªôt th·∫ø h·ªá Z thi·∫øu \n",
            "kinh nghi·ªám l√†m vi·ªác th·ª±c t·∫ø, coi tr·ªçng s·ª± ƒëa d·∫°ng v√† c√¥ng b·∫±ng, d·ªÖ d√†ng r∆°i v√†o tr·∫°ng th√°i lo √¢u v√† tr·∫ßm\n",
            "\n",
            "üìÑ Document 3:\n",
            "s·ª≠ d·ª•ng l·∫°i d·ªãch v·ª•, gi√∫p doanh ngh·ªáp c√≥ th·ªÉ n√¢ng cao doanh thu m√† kh√¥ng t·ªën nhi·ªÅu chi ph√≠.\n",
            "Th·∫ø h·ªá Z l√† th·∫ø h·ªá sinh ra trong giai ƒëo·∫°n c√¥ng ngh·ªá ph√°t tri·ªÉn, do ƒë√≥ th·∫ø h·ªá n√†y c√≥ kh·∫£ nƒÉng th√≠ch ·ª©ng \n",
            "nhanh ch√≥ng so v·ªõi c√°c th·∫ø h·ªá tr∆∞·ªõc ƒë√≥ khi Vi·ªát Nam ƒë·∫©y m·∫°nh c√°c ho·∫°t ƒë·ªông chuy·ªÉn ƒë·ªïi s·ªë  v√† ph√°t tri·ªÉn \n",
            "b·ªÅn v·ªØng. H∆°n th·∫ø n·ªØa, ƒë√¢y l√† th·∫ø h·ªá c√≥ ti·ªÅm nƒÉng ƒë√≥ng g√≥p r·∫•t l·ªõn cho s·ª± ph√°t tri·ªÉn c·ªßa n·ªÅn kinh t·∫ø trong\n",
            "\n",
            "üìÑ Document 4:\n",
            "S·ªë 330 th√°ng 12/2024 67\n",
            "Buchko, 2021), v√¨ v·∫≠y m√† c√°c doanh nghi·ªáp c·∫ßn ph·∫£i th·∫•u hi·ªÉu ƒë·ªÉ c√≥ th·ªÉ ƒë√°p ·ª©ng nh·ªØng nhu c·∫ßu c≈©ng nh∆∞ \n",
            "khai th√°c t·ªëi ƒëa ti·ªÅm nƒÉng c·ªßa th·∫ø h·ªá Z (Perilus, 2020).\n",
            "Trong th·ªùi gian g·∫ßn ƒë√¢y, c√°c nh√† nghi√™n c·ª©u b·∫Øt ƒë·∫ßu t·∫≠p trung h∆°n v√†o th·∫ø h·ªá Z ·ªü c√°c lƒ©nh v·ª±c nh∆∞ ti·∫øp \n",
            "th·ªã, du l·ªãch v√† kh·ªüi s·ª± kinh doanh (Doanh & Bernat, 2019; Nguyen & c·ªông s·ª±, 2022; Nguyen & c·ªông s·ª±, \n",
            "2021), nh∆∞ng v·∫´n ch∆∞a nhi·ªÅu c√°c nghi√™n c·ª©u t·∫≠p trung xem x√©t ƒë·ªëi t∆∞·ª£ng n√†y trong b·ªëi c·∫£nh c√¥ng vi·ªác ho·∫∑c\n"
          ]
        }
      ],
      "source": [
        "for i, doc in enumerate(retrieved_docs[:4]):  # Show top 3\n",
        "    print(f\"\\nüìÑ Document {i+1}:\\n{doc.page_content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xYlxyKNBV_p"
      },
      "source": [
        "#LLM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "j4VY2rLjmpA_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f53d161-a981-43dd-9201-9f82efaa520a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Chat model loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "chat_model = ChatGoogleGenerativeAI(\n",
        "    google_api_key=os.environ[\"GOOGLE_API_KEY\"],\n",
        "    model=\"gemini-2.0-flash-thinking-exp-01-21\",\n",
        "    temperature=0.7\n",
        ")\n",
        "print(\"‚úÖ Chat model loaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")"
      ],
      "metadata": {
        "id": "oUjvFMfNC1Ln"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n90n1D8qB9cR"
      },
      "source": [
        "#Augment_Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zXRBR2U7mziE"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")\n",
        "\n",
        "def augment_prompt(query):\n",
        "    # Get top 3 results from the knowledge base\n",
        "    results = vector_db.similarity_search(query, k=4)\n",
        "\n",
        "    # Extract text, sources, and pages\n",
        "    source_map = {}\n",
        "    for doc in results:\n",
        "        source = doc.metadata.get(\"source\", \"Unknown\")\n",
        "        page = doc.metadata.get(\"page\", \"Unknown\")\n",
        "        source_map[doc.page_content] = (source, page)\n",
        "\n",
        "    # Construct the augmented prompt\n",
        "    source_knowledge = \"\\n\".join(source_map.keys())\n",
        "\n",
        "    augmented_prompt = f\"\"\"B·∫°n l√† t∆∞ v·∫•n vi√™n c·ªßa tr∆∞·ªùng Sƒ© Quan Th√¥ng Tin. D·ª±a v√†o n·ªôi dung t√†i li·ªáu, h√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch ch√≠nh x√°c v√† th√¢n thi·ªán b·∫±ng ti·∫øng Vi·ªát.\n",
        "    Kh√¥ng th√™m th√¥ng tin ngo√†i n·ªôi dung t√†i li·ªáu. N·∫øu kh√¥ng t√¨m th·∫•y c√¢u tr·∫£ l·ªùi trong t√†i li·ªáu, ch·ªâ c·∫ßn n√≥i r·∫±ng b·∫°n kh√¥ng bi·∫øt.\n",
        "\n",
        "    N·ªôi dung t√†i li·ªáu:\n",
        "    {source_knowledge}\n",
        "\n",
        "    C√¢u h·ªèi:\n",
        "    {query}\"\"\"\n",
        "\n",
        "\n",
        "    return augmented_prompt, source_map\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdvgQND2CENP"
      },
      "source": [
        "#Answering Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxkV6QuupvcL"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# User question\n",
        "question = \"ƒê·ªô tu·ªïi Genz\"\n",
        "\n",
        "# Generate augmented prompt and retrieve sources\n",
        "context, source_map = augment_prompt(question)\n",
        "\n",
        "# Create human message for Gemini model\n",
        "prompt = HumanMessage(content=context)\n",
        "\n",
        "# Invoke Gemini model\n",
        "res = chat_model.invoke([prompt])\n",
        "response_text = res.content\n",
        "\n",
        "# Get embeddings for LLM response\n",
        "response_embedding = embedding_model.embed_query(response_text)\n",
        "\n",
        "# Track relevant sources\n",
        "relevant_sources = set()\n",
        "\n",
        "for text, (source, page) in source_map.items():\n",
        "    chunk_embedding = embedding_model.embed_query(text)  # Get embedding for each chunk\n",
        "    similarity_score = cosine_similarity([response_embedding], [chunk_embedding])[0][0]\n",
        "\n",
        "    if similarity_score >= 0.7:  # Threshold for relevance\n",
        "        relevant_sources.add(f\"{source} (Page {page+1})\")\n",
        "\n",
        "formatted_response = f\"Response: {response_text}\\nSources: {list(relevant_sources) if relevant_sources else ['No sources matched']}\"\n",
        "print(formatted_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6_KAOaqY0Ez"
      },
      "source": [
        "#Generate Questions and Save to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WhJAu7udPkJ"
      },
      "outputs": [],
      "source": [
        "'''import time\n",
        "import csv\n",
        "import os\n",
        "\n",
        "csv_filename = \"generated_questions.csv\"\n",
        "\n",
        "# Open CSV file for writing with UTF-8 BOM\n",
        "with open(csv_filename, mode=\"a\", newline=\"\", encoding=\"utf-8-sig\") as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Write the header (only \"Question\" column now)\n",
        "    writer.writerow([\"Question\"])\n",
        "\n",
        "    for i, doc in enumerate(pdf_files):\n",
        "        time.sleep(2)  # Avoid hitting API rate limits\n",
        "\n",
        "        # Extract document name (without path)\n",
        "        doc_name = os.path.basename(doc)\n",
        "\n",
        "        # Select relevant chunks\n",
        "        doc_chunks = chunks[i * 3 : (i + 1) * 3]\n",
        "        doc_text = \"\\n\".join([chunk.page_content.strip() for chunk in doc_chunks]).strip()\n",
        "\n",
        "        if not doc_text:\n",
        "            continue  # Skip if the document has no text\n",
        "\n",
        "        # Generate questions with improved prompt\n",
        "        prompt = HumanMessage(content=f\"\"\"D·ª±a v√†o n·ªôi dung t√†i li·ªáu \"{doc_name}\", h√£y t·∫°o 5 c√¢u h·ªèi ƒëa d·∫°ng b·∫±ng ti·∫øng Vi·ªát.\n",
        "        Kh√¥ng ƒë√°nh s·ªë th·ª© t·ª±, kh√¥ng ƒë·ªÉ l·∫°i kho·∫£ng tr·∫Øng d∆∞ th·ª´a, v√† m·ªói c√¢u h·ªèi ph·∫£i c√≥ ƒë·ªß ng·ªØ c·∫£nh ƒë·ªÉ hi·ªÉu ƒë∆∞·ª£c t√†i li·ªáu li√™n quan.\"\"\")\n",
        "\n",
        "        response = chat_model.invoke([prompt])\n",
        "        questions = [q.strip() for q in response.content.strip().split(\"\\n\") if q.strip()]  # Clean up questions\n",
        "\n",
        "        # Write each question to the CSV file\n",
        "        for question in questions:\n",
        "            writer.writerow([question])\n",
        "\n",
        "print(f\"‚úÖ Questions saved to {csv_filename}\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwAsxS4cZPOh"
      },
      "source": [
        "#Generate Answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccqKpRxNZO-_"
      },
      "outputs": [],
      "source": [
        "'''import pandas as pd\n",
        "import time\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "answer_csv = \"generated_QA.csv\"\n",
        "\n",
        "# Load generated questions\n",
        "df_questions = pd.read_csv(\"generated_questions.csv\")\n",
        "questions = df_questions[\"Question\"].tolist()\n",
        "\n",
        "answers = []\n",
        "answer_sources = []\n",
        "\n",
        "for question in questions:\n",
        "    time.sleep(2)  # Avoid hitting API rate limits\n",
        "\n",
        "    # Retrieve relevant document content\n",
        "    context, _ = augment_prompt(question)\n",
        "    prompt = HumanMessage(content=context)\n",
        "\n",
        "    # Get answer from chatbot model\n",
        "    answer_res = chat_model.invoke([prompt])\n",
        "    answer = answer_res.content.strip()\n",
        "    answers.append(answer)\n",
        "\n",
        "    # Identify relevant sources for the answer\n",
        "    relevant_sources = set()\n",
        "    response_embedding = embedding_model.embed_query(answer)  # Embed answer\n",
        "\n",
        "    for text, (source, page) in source_map.items():\n",
        "        chunk_embedding = embedding_model.embed_query(text)  # Embed document chunk\n",
        "        similarity_score = cosine_similarity([response_embedding], [chunk_embedding])[0][0]\n",
        "\n",
        "        if similarity_score >= 0.7:  # Threshold for relevance\n",
        "            relevant_sources.add(f\"{source} (Page {page+1})\")\n",
        "\n",
        "    # Store relevant sources for answer\n",
        "    source_list = \"; \".join(relevant_sources) if relevant_sources else \"No sources matched\"\n",
        "    answer_sources.append(source_list)\n",
        "\n",
        "# Save answers and sources to CSV\n",
        "df_answers = pd.DataFrame({\"Answer\": answers, \"Relevant Source (Answer)\": answer_sources})\n",
        "df_answers.to_csv(answer_csv, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(f\"‚úÖ Answers saved to {answer_csv}\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x3n1IQbCGND"
      },
      "source": [
        "#Reranker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "nU7jsd24CIUt"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "\n",
        "# Load PhoRanker model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"itdainb/PhoRanker\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ak-3DIP-C2TV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50d88055-e166-40bc-e601-005e09b7905e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade Pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "8bssNE7OChd3"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\"itdainb/PhoRanker\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "sl-9GLpbCKN0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def rerank_documents(query, docs):\n",
        "    scored_docs = []\n",
        "    for doc in docs:\n",
        "        candidate_text = doc.page_content\n",
        "        # Prepare the input pair (query, candidate)\n",
        "        inputs = tokenizer(query, candidate_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        # Get the score (assuming a single output neuron for relevance)\n",
        "        score = outputs.logits[0][0].item()  # Access the first element directly\n",
        "        scored_docs.append((doc, score))\n",
        "\n",
        "    # Sort the documents by score (highest first)\n",
        "    scored_docs = sorted(scored_docs, key=lambda x: x[1], reverse=True)\n",
        "    # Return only the document objects in sorted order\n",
        "    return [doc for doc, score in scored_docs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "iP_8p0keCVNJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "322343f6-4402-4093-9dc1-4df08af82e1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìÑ Document 1:\n",
            "S·ªë 330 th√°ng 12/2024 66\n",
            "1. Gi·ªõi thi·ªáu\n",
            "Th·∫ø h·ªá Z th∆∞·ªùng ƒë∆∞·ª£c c√°c nh√† nghi√™n c·ª©u x√°c ƒë·ªãnh l√† sinh ra trong kho·∫£ng th·ªùi gian t·ª´ 1995 ‚Äì 2012 \n",
            "(Barhate & Dirani, 2022; Maloni & c·ªông s·ª±, 2019). V·ªõi vi·ªác ƒë∆∞·ª£c ƒë√†o t·∫°o tr√¨nh ƒë·ªô ƒë·∫°i h·ªçc, nh√≥m n√†y ƒë√£ \n",
            "gia nh·∫≠p th·ªã tr∆∞·ªùng lao ƒë·ªông ƒë∆∞·ª£c kho·∫£ng 7 nƒÉm v√† ƒëang d·∫ßn tr·ªü th√†nh l·ª±c l∆∞·ª£ng lao ƒë·ªông ch√≠nh, ƒë·∫∑c bi·ªát l√† \n",
            "trong lƒ©nh v·ª±c kinh doanh v√† qu·∫£n l√Ω. Trong b·ªëi c·∫£nh chuy·ªÉn ƒë·ªïi l·ª±c l∆∞·ª£ng lao ƒë·ªông nh∆∞ v·∫≠y, r·∫•t c·∫ßn thi·∫øt\n",
            "\n",
            "üìÑ Document 2:\n",
            "2.2. Th·∫ø h·ªá Z\n",
            "L√† th·∫ø h·ªá m·ªõi nh·∫•t tham gia v√†o l·ª±c l∆∞·ª£ng lao ƒë·ªông, th·∫ø h·ªá Z cho th·∫•y s·ª± kh√°c bi·ªát ƒë√°ng k·ªÉ trong h√†nh \n",
            "vi v√† th√°i ƒë·ªô ƒë·ªëi v·ªõi c√¥ng vi·ªác so v·ªõi nh·ªØng th·∫ø h·ªá tr∆∞·ªõc ƒë√¢y. Vi·ªác c√≥ th·ªùi gian ƒëi h·ªçc d√†i, c√≥ s·ª± bao tr√πm \n",
            "c·ªßa c√¥ng ngh·ªá v√† thi·∫øt b·ªã di ƒë·ªông, ƒë∆∞·ª£c s·ªëng trong m·ªôt x√£ h·ªôi ph√°t tri·ªÉn h∆°n ƒë√£ t·∫°o ra m·ªôt th·∫ø h·ªá Z thi·∫øu \n",
            "kinh nghi·ªám l√†m vi·ªác th·ª±c t·∫ø, coi tr·ªçng s·ª± ƒëa d·∫°ng v√† c√¥ng b·∫±ng, d·ªÖ d√†ng r∆°i v√†o tr·∫°ng th√°i lo √¢u v√† tr·∫ßm\n",
            "\n",
            "üìÑ Document 3:\n",
            "s·ª≠ d·ª•ng l·∫°i d·ªãch v·ª•, gi√∫p doanh ngh·ªáp c√≥ th·ªÉ n√¢ng cao doanh thu m√† kh√¥ng t·ªën nhi·ªÅu chi ph√≠.\n",
            "Th·∫ø h·ªá Z l√† th·∫ø h·ªá sinh ra trong giai ƒëo·∫°n c√¥ng ngh·ªá ph√°t tri·ªÉn, do ƒë√≥ th·∫ø h·ªá n√†y c√≥ kh·∫£ nƒÉng th√≠ch ·ª©ng \n",
            "nhanh ch√≥ng so v·ªõi c√°c th·∫ø h·ªá tr∆∞·ªõc ƒë√≥ khi Vi·ªát Nam ƒë·∫©y m·∫°nh c√°c ho·∫°t ƒë·ªông chuy·ªÉn ƒë·ªïi s·ªë  v√† ph√°t tri·ªÉn \n",
            "b·ªÅn v·ªØng. H∆°n th·∫ø n·ªØa, ƒë√¢y l√† th·∫ø h·ªá c√≥ ti·ªÅm nƒÉng ƒë√≥ng g√≥p r·∫•t l·ªõn cho s·ª± ph√°t tri·ªÉn c·ªßa n·ªÅn kinh t·∫ø trong\n",
            "\n",
            "üìÑ Document 4:\n",
            "of Phoenix.\n",
            "Rudolph, C.W., & Zacher, H. (2017), ‚ÄòConsidering generations from a lifespan developmental perspective‚Äô, Work, \n",
            "Aging and Retirement, 3(2), 113-129. DOI: https://doi.org/10.1093/workar/waw019. \n",
            "Schroth, H. (2019), ‚ÄòAre you ready for Gen Z in the workplace?‚Äô, California Management Review, 61(3), 5-18. DOI: \n",
            "https://doi.org/10.1177/0008125619841006. \n",
            "Twenge, J.M., Campbell, S.M., Hoffman, B.J., & Lance, C.E. (2010), ‚ÄòGenerational differences in work values: Leisure\n",
            "\n",
            "üìÑ Document 5:\n",
            "S·ªë 330 th√°ng 12/2024 67\n",
            "Buchko, 2021), v√¨ v·∫≠y m√† c√°c doanh nghi·ªáp c·∫ßn ph·∫£i th·∫•u hi·ªÉu ƒë·ªÉ c√≥ th·ªÉ ƒë√°p ·ª©ng nh·ªØng nhu c·∫ßu c≈©ng nh∆∞ \n",
            "khai th√°c t·ªëi ƒëa ti·ªÅm nƒÉng c·ªßa th·∫ø h·ªá Z (Perilus, 2020).\n",
            "Trong th·ªùi gian g·∫ßn ƒë√¢y, c√°c nh√† nghi√™n c·ª©u b·∫Øt ƒë·∫ßu t·∫≠p trung h∆°n v√†o th·∫ø h·ªá Z ·ªü c√°c lƒ©nh v·ª±c nh∆∞ ti·∫øp \n",
            "th·ªã, du l·ªãch v√† kh·ªüi s·ª± kinh doanh (Doanh & Bernat, 2019; Nguyen & c·ªông s·ª±, 2022; Nguyen & c·ªông s·ª±, \n",
            "2021), nh∆∞ng v·∫´n ch∆∞a nhi·ªÅu c√°c nghi√™n c·ª©u t·∫≠p trung xem x√©t ƒë·ªëi t∆∞·ª£ng n√†y trong b·ªëi c·∫£nh c√¥ng vi·ªác ho·∫∑c\n",
            "\n",
            "üìÑ Document 6:\n",
            "org/10.1016/S1574-0714(06)01003-7. \n",
            "Farrell, W.C., & Phungsoonthorn, T. (2020), ‚ÄòGeneration Z in Thailand‚Äô, International Journal of Cross Cultural \n",
            "Management, 20(1), 25-51. DOI: https://doi.org/10.1177/1470595820904116. \n",
            "Gabrielova, K., & Buchko, A. A. (2021), ‚ÄòHere comes Generation Z: Millennials as managers‚Äô, Business Horizons, \n",
            "64(4), 489-499. DOI: https://doi.org/10.1016/j.bushor.2021.02.013.\n",
            "\n",
            "üìÑ Document 7:\n",
            "ph·∫£i t√¨m hi·ªÉu v·ªÅ nh·ªØng mong mu·ªën v√† k·ª≥ v·ªçng c·ªßa sinh vi√™n kh·ªëi ng√†nh kinh doanh v√† qu·∫£n l√Ω trong c√¥ng \n",
            "vi·ªác ‚Äì th·∫ø h·ªá Z ti·∫øp theo s·∫Ω b∆∞·ªõc v√†o th·ªã tr∆∞·ªùng lao ƒë·ªông.\n",
            "Nhi·ªÅu nghi√™n c·ª©u ƒë√£ ch·ªâ ra r·∫±ng, th·∫ø h·ªá Z c√≥ nh·ªØng ∆∞u ti√™n kh√°c bi·ªát so v·ªõi c√°c th·∫ø h·ªá tr∆∞·ªõc ƒë√¢y v·ªÅ c√¥ng \n",
            "vi·ªác, ch·∫≥ng h·∫°n nh∆∞ mong mu·ªën c√≥ s·ª± linh ho·∫°t trong c√¥ng vi·ªác, c∆° h·ªôi ph√°t tri·ªÉn c√° nh√¢n v√† t·∫ßm quan tr·ªçng \n",
            "c·ªßa c√¢n b·∫±ng gi·ªØa c√¥ng vi·ªác v√† cu·ªôc s·ªëng (Leslie & c·ªông s·ª±, 2021; Osorio & Madero, 2024). Tuy nhi√™n,\n",
            "\n",
            "üìÑ Document 8:\n",
            "T√†i li·ªáu tham kh·∫£o\n",
            "Barhate, B., & Dirani, K.M. (2022), ‚ÄòCareer aspirations of generation Z: a systematic literature review‚Äô, European \n",
            "Journal of Training and Development, 46(1/2), 139-157. DOI: 10.1108/EJTD-07-2020-0124.\n",
            "Becton, J.B., Walker, H.J., & Jones‚ÄêFarmer, A. (2014), ‚ÄòGenerational differences in workplace behavior‚Äô, Journal of \n",
            "Applied Social Psychology, 44(3), 175-189. DOI: 10.1111/jasp.12208.\n",
            "\n",
            "üìÑ Document 9:\n",
            "generation‚Äô, Journal of Business and Psychology, 25, 281-292. DOI 10.1007/s10869-010-9159-4. \n",
            "Nguyen, C., Nguyen, T., & Luu, V . (2022), ‚ÄòRelationship between influencer marketing and purchase intention: \n",
            "focusing on Vietnamese gen Z consumers‚Äô, Independent Journal of Management & Production, 13(2), 810-828. \n",
            "DOI: https://doi.org/10.14807/ijmp.v13i2.1603. \n",
            "Nguyen, V . H., Truong, T. X. D., Pham, H. T., Tran, D. T., & Nguyen, P. H. (2021), ‚ÄòTravel intention to visit tourism\n",
            "\n",
            "üìÑ Document 10:\n",
            "S·ªë 330 th√°ng 12/2024 70\n",
            "ƒë√°nh gi√° cao. ƒêi·ªÅu n√†y c√≥ th·ªÉ ƒë∆∞·ª£c l√Ω gi·∫£i b·ªüi th·∫ø h·ªá Z l√† m·ªôt th·∫ø h·ªá th·ª±c t·∫ø (Gabrielova & Buchko, 2021), \n",
            "h·ªç quan t√¢m ƒë·∫øn nh·ªØng y·∫øu t·ªë h·ªØu h√¨nh h∆°n l√† c√°c y·∫øu t·ªë v√¥ h√¨nh.\n",
            "M·∫∑c d√π kh√¥ng ƒë∆∞·ª£c x·∫øp h√†ng ƒë·∫ßu, nh∆∞ng nh√¨n chung c√°c gi√° tr·ªã c√¥ng vi·ªác ƒë·∫øn t·ª´ b√™n trong nh∆∞ s·ª± h·ªçc \n",
            "h·ªèi, n√¢ng cao k·ªπ nƒÉng, c√≥ th·ªÉ nh√¨n th·∫•y k·∫øt qu·∫£ l√†m vi·ªác v√† kh·∫£ nƒÉng s√°ng t·∫°o trong c√¥ng vi·ªác v·∫´n ƒë∆∞·ª£c coi\n",
            "\n",
            "üìÑ Document 11:\n",
            "destinations: A perspective of generation Z in Vietnam‚Äô, The Journal of Asian Finance, Economics and Business, \n",
            "8(2), 1043-1053. DOI: 10.13106/jafeb.2021.vol8.no2.1043. \n",
            "Osorio, M. L., & Madero, S. (2024), ‚ÄòExplaining Gen Z‚Äôs desire for hybrid work in corporate, family, and entrepreneurial \n",
            "settings‚Äô, Business Horizons, DOI: https://doi.org/10.1016/j.bushor.2024.02.008. \n",
            "Perilus, B. (2020), ‚ÄòEngaging four generations in the workplace: a single case study‚Äô, Doctoral dissertation, University \n",
            "of Phoenix.\n",
            "\n",
            "üìÑ Document 12:\n",
            "t∆∞∆°ng lai n√≥i chung v√† ng√†nh d·ªãch v·ª• n√≥i ri√™ng. Xu h∆∞·ªõng ti√™u d√πng c·ªßa th·∫ø h·ªá Z ƒë√£ v√† ƒëang thay ƒë·ªïi, ƒë√≤i \n",
            "h·ªèi s·ª± ƒë·ªïi m·ªõi s√°ng t·∫°o ƒë·ªÉ b·∫Øt k·ªãp v√† t·∫°o th·ªã tr∆∞·ªùng m·ªõi. V·∫≠y n√™n, nghi√™n c·ª©u v·ªÅ ƒë ·ªïi m·ªõi s√°ng t·∫°o d·ªãch v·ª• \n",
            "ƒë·ªëi v·ªõi kh√°ch h√†ng th·∫ø h·ªá Z c√≥ th·ªÉ ƒë∆∞·ª£c ·ª©ng d·ª•ng kh√¥ng ch·ªâ ·ªü hi·ªán t·∫°i m√† c√≤n ·ªü t∆∞∆°ng lai.\n",
            "Vai tr√≤ c·ªßa ƒë·ªïi m·ªõi s√°ng t·∫°o d·ªãch v·ª• ƒë√£ ƒë∆∞·ª£c kh√°m ph√° r·ªông r√£i trong c√°c nghi√™n c·ª©u tr∆∞·ªõc ƒë√¢y. Tuy \n",
            "nhi√™n, c√°c nghi√™n c·ª©u tr∆∞·ªõc ƒë√¢y th∆∞·ªùng ƒë√°nh gi√° t√°c ƒë·ªông c·ªßa ƒë·ªïi m·ªõi s√°ng t·∫°o d·ªãch v·ª• ƒë·ªëi v·ªõi s·ª± h√†i l√≤ng\n",
            "\n",
            "üìÑ Document 13:\n",
            "m∆°ÃÅi s√°ng t·∫°o ph√π h·ª£p v∆°ÃÅi nhu c·∫ßu v√† ƒë·∫°ÃÜc ƒëi·ªÉm c·ªßa t·ª´ng nh√≥m kh√°ch h√†ng th·∫ø h·ªá Z, th·∫ø h·ªá ti√™u \n",
            "d√πng t∆∞∆°ng lai t·∫°i Vi·ªát Nam.\n",
            "T·ª´ kho√°: ƒê·ªïi m·ªõi d·ªãch v·ª•, s·ª± h√†i l√≤ng c·ªßa kh√°ch h√†ng, th·∫ø h·ªá Z, √Ω ƒë·ªãnh mua l·∫°i.\n",
            "M√£ JED: M1, M21\n",
            "The Influence of Service Innovation on The Satisfaction and Repurchases Intentions of \n",
            "Generation Z Customers\n",
            "Abstract\n",
            "This study aims to evaluate the impact of service innovation, including supportive service\n",
            "\n",
            "üìÑ Document 14:\n",
            "Vi·ªát Nam ƒë·∫∑t l√†m ∆∞u ti√™n trong qu√° tr√¨nh t√¨m ki·∫øm vi·ªác l√†m. S·ª± ∆∞u ti√™n h√†ng ƒë·∫ßu c·ªßa sinh vi√™n th·∫ø h·ªá Z l√† \n",
            "c√¥ng vi·ªác ƒëem l·∫°i m·ª©c l∆∞∆°ng cao v√† l·ªô tr√¨nh thƒÉng ti·∫øn r√µ r√†ng. ƒêi·ªÅu n√†y kh√¥ng ch·ªâ ph·∫£n √°nh mong mu·ªën \n",
            "v·ªÅ s·ª± ƒë·∫£m b·∫£o t√†i ch√≠nh m√† c√≤n cho th·∫•y s·ª± quan t√¢m ƒë·∫øn vi·ªác ph√°t tri·ªÉn c√° nh√¢n trong s·ª± nghi·ªáp. V√† kh√¥ng \n",
            "ch·ªâ th·∫ø h·ªá Z m√† c√°c gi√° tr·ªã b√™n ngo√†i v·∫´n lu√¥n ƒë∆∞·ª£c c√°c th·∫ø h·ªá tr∆∞·ªõc nh∆∞ l√† th·∫ø h·ªá X v√† th·∫ø h·ªá Y k·ª≥ v·ªçng\n",
            "\n",
            "üìÑ Document 15:\n",
            "r·∫±ng th·∫ø h·ªá Z coi tr·ªçng ph√∫c l·ª£i, k·∫ø ho·∫°ch h∆∞u tr√≠ v√† kh·∫£ nƒÉng d·ª± ƒëo√°n s·ª± thay ƒë·ªïi, v√† h∆°n c·∫£ l√† ph√≠a doanh \n",
            "nghi·ªáp c≈©ng ƒë·ªìng t√¨nh v·ªÅ v·∫•n ƒë·ªÅ n√†y. ƒêi·ªÅu n√†y kh√¥ng ch·ªâ xu·∫•t hi·ªán ·ªü n·ªÅn kinh t·∫ø m·ªõi n·ªïi nh∆∞ Vi·ªát Nam m√† \n",
            "ƒë√£ xu·∫•t hi·ªán trong b·ªëi c·∫£nh nghi√™n c·ª©u ·ªü c√°c qu·ªëc gia ph√°t tri·ªÉn, gi√° tr·ªã c√¥ng vi·ªác v·ªÅ s·ª± ·ªïn ƒë·ªãnh ·ªü ƒë√¢y ƒë∆∞·ª£c \n",
            "th·∫ø h·ªá Z r·∫•t coi tr·ªçng (Maloni & c·ªông s·ª±, 2019). C·∫ßn l∆∞u √Ω r·∫±ng, vi·ªác ƒë·ªÅ cao t√≠nh ·ªïn ƒë·ªãnh v√† v·∫•n ƒë·ªÅ th∆∞·ªùng\n",
            "\n",
            "üìÑ Document 16:\n",
            "nh·ªØng k·∫øt n·ªëi c·ªßa th·∫ø h·ªá Z hi·ªán nay ƒë∆∞·ª£c thi·∫øt l·∫≠p tr√™n c√°c n·ªÅn t·∫£ng tr·ª±c tuy·∫øn (Farrell & Phungsoonthorn, \n",
            "2020; Schroth, 2019), v√¨ v·∫≠y m√† h·ªç √≠t quan t√¢m h∆°n ƒë·∫øn nh·ªØng gi√° tr·ªã k·∫øt n·ªëi x√£ h·ªôi ·ªü n∆°i l√†m vi·ªác. Ngo√†i \n",
            "ra, k·∫øt qu·∫£ c≈©ng c√≥ th·ªÉ l√† g·ª£i √Ω cho s·ª± thay ƒë·ªïi gi√° tr·ªã x√£ h·ªôi v√† th·ªùi gian ngh·ªâ ng∆°i c·ªßa th·∫ø h·ªá Z v√† c·∫ßn ƒë∆∞·ª£c \n",
            "kh√°m ph√° th√™m.\n",
            "Kh√°c v·ªõi nh·∫≠n ƒë·ªãnh th√¥ng th∆∞·ªùng r·∫±ng th·∫ø h·ªá Z √≠t quan t√¢m t·ªõi s·ª± ·ªïn ƒë·ªãnh, k·∫øt qu·∫£ nghi√™n c·ª©u ch·ªâ ra\n",
            "\n",
            "üìÑ Document 17:\n",
            "c·∫£m, thi·∫øu k·ªπ nƒÉng giao ti·∫øp (Schroth, 2019). Kh√¥ng ch·ªâ kh√°c bi·ªát v·ªõi c√°c th·∫ø h·ªá kh√°c, ngay trong th·∫ø h·ªá Z \n",
            "c≈©ng c√≥ s·ª± kh√°c bi·ªát ƒë√°ng k·ªÉ v·ªõi nh·ªØng nh·∫≠n th·ª©c v√† gi√° tr·ªã kh√°c nhau nh∆∞: c√¢n b·∫±ng cu·ªôc s·ªëng-c√¥ng vi·ªác, \n",
            "m√¥i tr∆∞·ªùng l√†m vi·ªác tho·∫£i m√°i v√† s·ª± thƒÉng ti·∫øn trong ngh·ªÅ nghi·ªáp (Leslie & c·ªông s·ª±, 2021). ƒêi·ªÅu n√†y c√≥ th·ªÉ \n",
            "d·∫´n ƒë·∫øn nh·ªØng xung ƒë·ªôt ti·ªÅm t√†ng trong doanh nghi·ªáp gi·ªØa qu·∫£n l√Ω v√† nh√¢n vi√™n th·∫ø h·ªá Z (Gabrielova &\n",
            "\n",
            "üìÑ Document 18:\n",
            "c≈©ng t·∫≠p trung ch·ªâ ra c√°c chi·∫øn l∆∞·ª£c ph√°t tri·ªÉn con ng∆∞·ªùi ri√™ng cho t·ª´ng th·∫ø h·ªá v√† c√°ch k·∫øt h·ª£p c√°c th·∫ø h·ªá \n",
            "v·ªõi nhau ƒë·ªÉ t·∫°o ra gi√° tr·ªã cho doanh nghi·ªáp (Callanan & Greenhaus, 2008; Chaudhuri & Ghosh, 2012; Ng \n",
            "& c·ªông s·ª±, 2010).\n",
            "Tuy nhi√™n, c√°c nghi√™n c·ª©u s·ª≠ d·ª•ng L√Ω thuy·∫øt Th·∫ø h·ªá th∆∞·ªùng t·∫≠p trung v√†o s·ª± kh√°c bi·ªát v√† k·∫øt n·ªëi gi·ªØa c√°c \n",
            "th·∫ø h·ªá b√™n trong doanh nghi·ªáp m√† c√≤n √≠t xem x√©t ƒë·∫øn tr∆∞·ªùng h·ª£p s·ª± kh√°c bi·ªát c·ªßa nh·ªØng ng∆∞·ªùi b√™n trong \n",
            "doanh nghi·ªáp v√† nh·ªØng ng∆∞·ªùi s·∫Øp b∆∞·ªõc ch√¢n v√†o doanh nghi·ªáp.\n",
            "2.2. Th·∫ø h·ªá Z\n",
            "\n",
            "üìÑ Document 19:\n",
            "Hampton, D., & Welsh, D. (2019), ‚ÄòWork values of Generation Z nurses‚Äô, JONA: The Journal of Nursing Administration, \n",
            "49(10), 480-486. DOI: 10.1097/NNA.0000000000000791. \n",
            "Hansen, J.-I. C., & Leuty, M.E. (2012), ‚ÄòWork values across generations‚Äô, Journal of Career Assessment, 20(1), 34-52. \n",
            "DOI: https://doi.org/10.1177/1069072711417163. \n",
            "Hurst, J.L., & Good, L.K. (2009), ‚ÄòGeneration Y and career choice: The impact of retail career perceptions,\n",
            "\n",
            "üìÑ Document 20:\n",
            "ng∆∞·ª£c trong nh√¨n nh·∫≠n c√°c kh√≠a c·∫°nh c·ªßa qu·∫£n l√Ω tr·ª±c ti·∫øp c√≤n c√≥ th·ªÉ l√Ω gi·∫£i b·ªüi nh·ªØng kh√°c bi·ªát trong quan \n",
            "ƒëi·ªÉm gi√° tr·ªã c√¥ng vi·ªác c·ªßa th·∫ø h·ªá Y (ƒëang l√† qu·∫£n l√Ω) v√† th·∫ø h·ªá Z (ƒëang l√† nh√¢n vi√™n) (Gabrielova & Buchko, \n",
            "2021).\n",
            "Thay v√¨ s·ª± kh√°c bi·ªát ƒë∆∞·ª£c t·∫°o th√†nh tr√™n c∆° s·ªü sinh vi√™n ƒë√°nh gi√° gi√° tr·ªã cao h∆°n doanh nghi·ªáp, th√¨ k·∫øt \n",
            "qu·∫£ l·∫°i cho th·∫•y ƒëi·ªÅu ng∆∞·ª£c l·∫°i. ·ªû c√°c gi√° tr·ªã v·ªÅ x√£ h·ªôi v√† th·ªùi gian ngh·ªâ ng∆°i, d∆∞·ªùng nh∆∞ doanh nghi·ªáp ƒëang\n",
            "-------------------------------------------------------------------------\n",
            "\n",
            "üìÑ Document 1:\n",
            "S·ªë 330 th√°ng 12/2024 66\n",
            "1. Gi·ªõi thi·ªáu\n",
            "Th·∫ø h·ªá Z th∆∞·ªùng ƒë∆∞·ª£c c√°c nh√† nghi√™n c·ª©u x√°c ƒë·ªãnh l√† sinh ra trong kho·∫£ng th·ªùi gian t·ª´ 1995 ‚Äì 2012 \n",
            "(Barhate & Dirani, 2022; Maloni & c·ªông s·ª±, 2019). V·ªõi vi·ªác ƒë∆∞·ª£c ƒë√†o t·∫°o tr√¨nh ƒë·ªô ƒë·∫°i h·ªçc, nh√≥m n√†y ƒë√£ \n",
            "gia nh·∫≠p th·ªã tr∆∞·ªùng lao ƒë·ªông ƒë∆∞·ª£c kho·∫£ng 7 nƒÉm v√† ƒëang d·∫ßn tr·ªü th√†nh l·ª±c l∆∞·ª£ng lao ƒë·ªông ch√≠nh, ƒë·∫∑c bi·ªát l√† \n",
            "trong lƒ©nh v·ª±c kinh doanh v√† qu·∫£n l√Ω. Trong b·ªëi c·∫£nh chuy·ªÉn ƒë·ªïi l·ª±c l∆∞·ª£ng lao ƒë·ªông nh∆∞ v·∫≠y, r·∫•t c·∫ßn thi·∫øt\n",
            "\n",
            "üìÑ Document 2:\n",
            "s·ª≠ d·ª•ng l·∫°i d·ªãch v·ª•, gi√∫p doanh ngh·ªáp c√≥ th·ªÉ n√¢ng cao doanh thu m√† kh√¥ng t·ªën nhi·ªÅu chi ph√≠.\n",
            "Th·∫ø h·ªá Z l√† th·∫ø h·ªá sinh ra trong giai ƒëo·∫°n c√¥ng ngh·ªá ph√°t tri·ªÉn, do ƒë√≥ th·∫ø h·ªá n√†y c√≥ kh·∫£ nƒÉng th√≠ch ·ª©ng \n",
            "nhanh ch√≥ng so v·ªõi c√°c th·∫ø h·ªá tr∆∞·ªõc ƒë√≥ khi Vi·ªát Nam ƒë·∫©y m·∫°nh c√°c ho·∫°t ƒë·ªông chuy·ªÉn ƒë·ªïi s·ªë  v√† ph√°t tri·ªÉn \n",
            "b·ªÅn v·ªØng. H∆°n th·∫ø n·ªØa, ƒë√¢y l√† th·∫ø h·ªá c√≥ ti·ªÅm nƒÉng ƒë√≥ng g√≥p r·∫•t l·ªõn cho s·ª± ph√°t tri·ªÉn c·ªßa n·ªÅn kinh t·∫ø trong\n",
            "\n",
            "üìÑ Document 3:\n",
            "S·ªë 330 th√°ng 12/2024 67\n",
            "Buchko, 2021), v√¨ v·∫≠y m√† c√°c doanh nghi·ªáp c·∫ßn ph·∫£i th·∫•u hi·ªÉu ƒë·ªÉ c√≥ th·ªÉ ƒë√°p ·ª©ng nh·ªØng nhu c·∫ßu c≈©ng nh∆∞ \n",
            "khai th√°c t·ªëi ƒëa ti·ªÅm nƒÉng c·ªßa th·∫ø h·ªá Z (Perilus, 2020).\n",
            "Trong th·ªùi gian g·∫ßn ƒë√¢y, c√°c nh√† nghi√™n c·ª©u b·∫Øt ƒë·∫ßu t·∫≠p trung h∆°n v√†o th·∫ø h·ªá Z ·ªü c√°c lƒ©nh v·ª±c nh∆∞ ti·∫øp \n",
            "th·ªã, du l·ªãch v√† kh·ªüi s·ª± kinh doanh (Doanh & Bernat, 2019; Nguyen & c·ªông s·ª±, 2022; Nguyen & c·ªông s·ª±, \n",
            "2021), nh∆∞ng v·∫´n ch∆∞a nhi·ªÅu c√°c nghi√™n c·ª©u t·∫≠p trung xem x√©t ƒë·ªëi t∆∞·ª£ng n√†y trong b·ªëi c·∫£nh c√¥ng vi·ªác ho·∫∑c\n",
            "\n",
            "üìÑ Document 4:\n",
            "org/10.1016/S1574-0714(06)01003-7. \n",
            "Farrell, W.C., & Phungsoonthorn, T. (2020), ‚ÄòGeneration Z in Thailand‚Äô, International Journal of Cross Cultural \n",
            "Management, 20(1), 25-51. DOI: https://doi.org/10.1177/1470595820904116. \n",
            "Gabrielova, K., & Buchko, A. A. (2021), ‚ÄòHere comes Generation Z: Millennials as managers‚Äô, Business Horizons, \n",
            "64(4), 489-499. DOI: https://doi.org/10.1016/j.bushor.2021.02.013.\n"
          ]
        }
      ],
      "source": [
        "query = \"GenZ sinh ra trong kho·∫£ng th·ªùi gian n√†o?\"\n",
        "# Retrieve candidates (increase k to get more candidates for reranking)\n",
        "retrieved_docs = vector_db.similarity_search(query, k=20)\n",
        "\n",
        "# Rerank the documents using PhoRanker\n",
        "reranked_docs = rerank_documents(query, retrieved_docs)\n",
        "\n",
        "# Now you can use the top reranked documents (e.g., top 3)\n",
        "top_docs = reranked_docs[:4]\n",
        "\n",
        "# Print out the top documents for inspection\n",
        "for i, doc in enumerate(retrieved_docs):\n",
        "    print(f\"\\nüìÑ Document {i+1}:\\n{doc.page_content}\")\n",
        "\n",
        "print(\"-------------------------------------------------------------------------\")\n",
        "for i, doc in enumerate(top_docs):\n",
        "    print(f\"\\nüìÑ Document {i+1}:\\n{doc.page_content}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_prompt_with_reranker(query):\n",
        "    # Retrieve a larger set of candidate docs\n",
        "    candidates = vector_db.similarity_search(query, k=15)\n",
        "    # Rerank them using PhoRanker\n",
        "    reranked_candidates = rerank_documents(query, candidates)\n",
        "\n",
        "    # Ch·ªçn top 4 t√†i li·ªáu sau khi rerank\n",
        "    selected_docs = reranked_candidates[:4]\n",
        "\n",
        "    # T·∫°o source map v√† tr√≠ch d·∫´n ngu·ªìn\n",
        "    source_map = {}\n",
        "    formatted_sources = []\n",
        "\n",
        "    for idx, doc in enumerate(selected_docs, start=1):\n",
        "        source = doc.metadata.get(\"source\", \"Unknown\")\n",
        "        page = doc.metadata.get(\"page\", \"Unknown\")\n",
        "        source_map[doc.page_content] = (source, page)\n",
        "        formatted_sources.append(f\"[{idx}] {source} - Trang {page}\")\n",
        "\n",
        "    # N·ªôi dung t√†i li·ªáu d√πng ƒë·ªÉ tr·∫£ l·ªùi\n",
        "    source_knowledge = \"\\n\\n\".join(f\"({i+1}) {doc.page_content}\" for i, doc in enumerate(selected_docs))\n",
        "    citation_info = \"\\n\".join(formatted_sources)\n",
        "\n",
        "    # Prompt t·ªëi ∆∞u\n",
        "    augmented_prompt = f\"\"\"B·∫°n l√† t∆∞ v·∫•n vi√™n c·ªßa tr∆∞·ªùng VGU. H√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch ch√≠nh x√°c, th√¢n thi·ªán, v√† tr√≠ch d·∫´n ngu·ªìn g·ªëc th√¥ng tin.\n",
        "B·∫°n ch·ªâ ƒë∆∞·ª£c s·ª≠ d·ª•ng th√¥ng tin t·ª´ t√†i li·ªáu d∆∞·ªõi ƒë√¢y, kh√¥ng th√™m n·ªôi dung kh√¥ng c√≥ trong t√†i li·ªáu. N·∫øu kh√¥ng t√¨m th·∫•y c√¢u tr·∫£ l·ªùi, ch·ªâ c·∫ßn n√≥i r·∫±ng b·∫°n kh√¥ng bi·∫øt.\n",
        "\n",
        "üìå **N·ªôi dung t√†i li·ªáu tr√≠ch xu·∫•t**:\n",
        "{source_knowledge}\n",
        "\n",
        "‚ùì **C√¢u h·ªèi**:\n",
        "{query}\n",
        "\n",
        "üìñ **Ngu·ªìn t√†i li·ªáu**:\n",
        "{citation_info}\n",
        "\"\"\"\n",
        "\n",
        "    print(candidates)\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "    return augmented_prompt, source_map\n"
      ],
      "metadata": {
        "id": "yGEx-BMycsva"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import CrossEncoder  # ‚úÖ NEW\n",
        "import numpy as np\n",
        "\n",
        "# Load Cross-Encoder model\n",
        "cross_encoder = CrossEncoder(\"itdainb/PhoRanker\")"
      ],
      "metadata": {
        "id": "7hYuR-BVLxwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # ‚úÖ Use a ranking model\n",
        "\n",
        "# User question\n",
        "question = \"ƒë·ªô tu·ªïi GenZ\"\n",
        "\n",
        "# Retrieve chunks & sources\n",
        "context, source_map = augment_prompt_with_reranker(question)\n",
        "\n",
        "# Invoke LLM\n",
        "prompt = HumanMessage(content=context)\n",
        "res = chat_model.invoke([prompt])\n",
        "response_text = res.content  # LLM-generated response\n",
        "'''\n",
        "# Track relevant sources\n",
        "relevant_sources = set()\n",
        "\n",
        "# Score each retrieved chunk using Cross-Encoder\n",
        "scores = {}\n",
        "for text, (source, page) in source_map.items():\n",
        "    score = cross_encoder.predict([(question, text)])[0]  # ‚úÖ Cross-Encoder scoring\n",
        "    scores[(source, page)] = score\n",
        "\n",
        "# Sort & filter sources by relevance threshold\n",
        "sorted_sources = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "for (source, page), score in sorted_sources:\n",
        "    if score >= 0.5:  # ‚úÖ Adjust threshold as needed\n",
        "        relevant_sources.add(f\"{source} (Page {page+1})\")\n",
        "\n",
        "# Format response\n",
        "formatted_response = f\"Response: {response_text}\\nSources: {list(relevant_sources) if relevant_sources else ['No sources matched']}\"\n",
        "print(formatted_response)'''\n",
        "print(response_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "Jl6zEFfgc__t",
        "outputId": "47290475-b63e-429f-c332-81a6d6cf2a55"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'vector_db' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-3c8a9b366767>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Retrieve chunks & sources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugment_prompt_with_reranker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Invoke LLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-4c8738ad666d>\u001b[0m in \u001b[0;36maugment_prompt_with_reranker\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maugment_prompt_with_reranker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Retrieve a larger set of candidate docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvector_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Rerank them using PhoRanker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mreranked_candidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrerank_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vector_db' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VrLvxLhtLe12"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMYO5NCpsC6d3liqmTatl8G",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}