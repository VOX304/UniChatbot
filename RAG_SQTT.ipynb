{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMBuFLjwu2N301ZcEvYFSdU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VOX304/SchoolChatbot/blob/main/RAG_SQTT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Packages setting up"
      ],
      "metadata": {
        "id": "X15KmGsrA-b9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install langchain \\\n",
        "langchain_community \\\n",
        "langchain_core \\\n",
        "langchain_google_genai \\\n",
        "python-dotenv \\\n",
        "pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T57Y2pVRXf6E",
        "outputId": "7b5956d7-0e49-406d-fd78-3679982af271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.20)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.19-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.11/dist-packages (0.3.43)\n",
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-2.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-5.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.13)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (4.12.2)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain_google_genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.16 (from langchain_google_genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.16-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (2.24.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.26.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (4.25.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.69.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.62.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain_community-0.3.19-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_google_genai-2.1.0-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading pypdf-5.3.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.16-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: filetype, python-dotenv, pypdf, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, google-ai-generativelanguage, langchain_google_genai, langchain_community\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.4 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.16 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 filetype-1.2.0 google-ai-generativelanguage-0.6.16 httpx-sse-0.4.0 langchain_community-0.3.19 langchain_google_genai-2.1.0 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.8.1 pypdf-5.3.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "452fa8cc3d6a4a47a83ee126dfe36c42"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLaao_cDlUls",
        "outputId": "942b9cf1-191e-4fde-d72e-01aa680a0dda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install scikit-learn \\\n",
        "numpy"
      ],
      "metadata": {
        "id": "v2ZZGF2_Bc-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
        "pdf_files = ['/content/20240918_466_QD-DHVD_Ban hanh Quy che dao tao Nam dai cuong cua VGU-VN.pdf',\n",
        "             '/content/CSE2021_Info Session  Internship, Thesis and Graduation.pdf',\n",
        "             '/content/CSE_Specific Examination Regulation_Annex A and B_VN.pdf',\n",
        "             '/content/General Examination Regulation for Bachelor and Master programs_VN.pdf']  # Adjust paths"
      ],
      "metadata": {
        "id": "cmFEYpAKhjHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PDF-Preprocessing & VectorDB"
      ],
      "metadata": {
        "id": "SfRZiKG73qBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = []\n",
        "for pdf in pdf_files:\n",
        "    pdf_loader = PyPDFLoader(pdf)\n",
        "    documents.extend(pdf_loader.load())\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "# Ensure embeddings are generated correctly\n",
        "#embeddings = embedding_model.embed_documents([doc.page_content for doc in chunks])\n",
        "\n",
        "# Pass embedded vectors to FAISS\n",
        "\n",
        "with open(\"extracted_content.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        f.write(f\"Chunk {i+1}:\\n{chunk.page_content}\\n\\n{'='*50}\\n\\n\")\n",
        "\n",
        "print(\"ğŸ“„ Extracted content saved to extracted_content.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9FNPA0Imh3TV",
        "outputId": "d3abec44-bc1f-4ce4-bbdf-adc19a5f4926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“„ Extracted content saved to extracted_content.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_db = FAISS.from_documents(chunks, embedding_model)"
      ],
      "metadata": {
        "id": "zVYVQzMHlXbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"âœ… Processed {len(chunks)} text chunks into FAISS vector database.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_Zy1uOzh0Yw",
        "outputId": "48ee9410-fb5d-435c-8ade-7315b8e38883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Processed 605 text chunks into FAISS vector database.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the requirement for graduation?\"\n",
        "retrieved_docs = vector_db.similarity_search(query, k = 3)\n"
      ],
      "metadata": {
        "id": "S6DA2G3NmVFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, doc in enumerate(retrieved_docs[:3]):  # Show top 3\n",
        "    print(f\"\\nğŸ“„ Document {i+1}:\\n{doc.page_content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AL0_aPymcvs",
        "outputId": "509d4e5d-a462-4669-e72c-2bf5fe805325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“„ Document 1:\n",
            "GRADUATION\n",
            "1. General Information\n",
            "2. Graduation Timeline\n",
            "\n",
            "ğŸ“„ Document 2:\n",
            "Vietnamese-German University Computer Science Program\n",
            "General Information\n",
            "1. Prerequisites: \n",
            "- Pass all modules (180 ECTS)\n",
            "- Complete 04 German classes or submit an A2 German Certificate\n",
            "2. Expected timeline:\n",
            "- VGU conducts two graduation assessments annually: in April and October \n",
            "- Only one Graduation Ceremony: November\n",
            "\n",
            "ğŸ“„ Document 3:\n",
            "General Information\n",
            "1. Prerequisites: \n",
            "- Evidence of the internship registration with a signed training contract (IC)\n",
            "- Successful completion of all modules of the first 5 semesters (150 ECTS)\n",
            "2. Grading Policy: Bachelor Thesis (weighting 80%) and Colloquium \n",
            "(min. 30 min. and max. 60 min., weighting 20%)\n",
            "3. Regulation: Thesis final reports submitted late will fail. Bachelorâ€™s \n",
            "thesis with colloquium only be repeated once.\n",
            "Vietnamese-German University\n",
            "7\n",
            "Computer Science Program\n",
            "\n",
            "ğŸ“„ Document 4:\n",
            "Vietnamese-German University Computer Science Program\n",
            "Graduation Timeline\n",
            "1. Graduation Assessment:\n",
            "- October 2025 graduation list: Complete the Oral Defense before August 30, 2025\n",
            "- April 2026 graduation list: Complete the Oral Defense before February 20, 2026 \n",
            "2. Status Fee:\n",
            "Students not eligible to start their final thesis in the expected semester are still required \n",
            "to pay regular tuition fees for that semester. \n",
            "If the final thesis is not submitted before November 30, 2025 (for the thesis registered\n",
            "\n",
            "ğŸ“„ Document 5:\n",
            "Grading policy \n",
            "Assessment method Percentage of total Assessment date \n",
            "Final exam 100  \n",
            "Total 100\n",
            "\n",
            "ğŸ“„ Document 6:\n",
            "Grading policy \n",
            "Assessment method Percentage of total Assessment date \n",
            "Final exam 100  \n",
            "Total 100\n",
            "\n",
            "ğŸ“„ Document 7:\n",
            "Exams  \n",
            "- Final written exam 90 minutes.  \n",
            "- Prerequisites for module examination: successful completion of the group work. \n",
            "- No materials, reference are allowed in the final exam room. \n",
            " \n",
            " \n",
            "Grading policy \n",
            "Assessment method Percentage of total Assessment date \n",
            "Final exam 100  \n",
            "Total 100\n",
            "\n",
            "ğŸ“„ Document 8:\n",
            "Grading policy \n",
            "Assessment method Percentage of total Assessment date \n",
            "Active participation 50  \n",
            "Project report 35  \n",
            "Oral Presentation 15  \n",
            "Total 100\n",
            "\n",
            "ğŸ“„ Document 9:\n",
            "General Information\n",
            "1. Prerequisites: student obtains 120 ECTS\n",
            "2. Requirements:\n",
            "â€¢Duration: at least 14 weeks (03 months). \n",
            "â€¢Full-time job: 05 days per week, 08 hours/day.\n",
            "â€¢Starts in the 8th semester (SS2025, after the Tet holiday)\n",
            "3. Grading Policy: Internship report (80%) + Presentation (20%)\n",
            "Vietnamese-German University\n",
            "2\n",
            "Computer Science Program\n",
            "\n",
            "ğŸ“„ Document 10:\n",
            "documentation. The program has to be correct with respect to its specification. \n",
            " \n",
            "Grading policy \n",
            "Assessment method Percentage of total Assessment date \n",
            "Assignments 20  \n",
            "Project 80  \n",
            "Total 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LLM model"
      ],
      "metadata": {
        "id": "0xYlxyKNBV_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "chat_model = ChatGoogleGenerativeAI(\n",
        "    google_api_key=os.environ[\"GOOGLE_API_KEY\"],\n",
        "    model=\"gemini-2.0-flash-thinking-exp-01-21\",\n",
        "    temperature=0.7\n",
        ")\n",
        "print(\"âœ… Chat model loaded successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4VY2rLjmpA_",
        "outputId": "be51b47e-185b-4245-ce6a-bf7c9e5248d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Chat model loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Augment_Prompt"
      ],
      "metadata": {
        "id": "n90n1D8qB9cR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")\n",
        "\n",
        "def augment_prompt(query):\n",
        "    # Get top 3 results from the knowledge base\n",
        "    results = vector_db.similarity_search(query, k=4)\n",
        "\n",
        "    # Extract text, sources, and pages\n",
        "    source_map = {}\n",
        "    for doc in results:\n",
        "        source = doc.metadata.get(\"source\", \"Unknown\")\n",
        "        page = doc.metadata.get(\"page\", \"Unknown\")\n",
        "        source_map[doc.page_content] = (source, page)\n",
        "\n",
        "    # Construct the augmented prompt\n",
        "    source_knowledge = \"\\n\".join(source_map.keys())\n",
        "\n",
        "    augmented_prompt = f\"\"\"Báº¡n lÃ  tÆ° váº¥n viÃªn cá»§a trÆ°á»ng SÄ© Quan ThÃ´ng Tin. Dá»±a vÃ o ná»™i dung tÃ i liá»‡u, hÃ£y tráº£ lá»i cÃ¢u há»i má»™t cÃ¡ch chÃ­nh xÃ¡c vÃ  thÃ¢n thiá»‡n báº±ng tiáº¿ng Viá»‡t.\n",
        "    KhÃ´ng thÃªm thÃ´ng tin ngoÃ i ná»™i dung tÃ i liá»‡u. Náº¿u khÃ´ng tÃ¬m tháº¥y cÃ¢u tráº£ lá»i trong tÃ i liá»‡u, chá»‰ cáº§n nÃ³i ráº±ng báº¡n khÃ´ng biáº¿t.\n",
        "\n",
        "    Ná»™i dung tÃ i liá»‡u:\n",
        "    {source_knowledge}\n",
        "\n",
        "    CÃ¢u há»i:\n",
        "    {query}\"\"\"\n",
        "\n",
        "\n",
        "    return augmented_prompt, source_map\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zXRBR2U7mziE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test - generating answer based on knowledge base"
      ],
      "metadata": {
        "id": "PdvgQND2CENP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# User question\n",
        "question = \"Theo thÃ´ng tin tá»« buá»•i Info Session CSE2021, sinh viÃªn CSE cáº§n Ä‘Ã¡p á»©ng nhá»¯ng yÃªu cáº§u há»c táº­p chÃ­nh nÃ o ngoÃ i khÃ³a luáº­n tá»‘t nghiá»‡p Ä‘á»ƒ Ä‘á»§ Ä‘iá»u kiá»‡n tá»‘t nghiá»‡p?\"\n",
        "\n",
        "# Generate augmented prompt and retrieve sources\n",
        "context, source_map = augment_prompt(question)\n",
        "\n",
        "# Create human message for Gemini model\n",
        "prompt = HumanMessage(content=context)\n",
        "\n",
        "# Invoke Gemini model\n",
        "res = chat_model.invoke([prompt])\n",
        "response_text = res.content\n",
        "\n",
        "# Get embeddings for LLM response\n",
        "response_embedding = embedding_model.embed_query(response_text)\n",
        "\n",
        "# Track relevant sources\n",
        "relevant_sources = set()\n",
        "\n",
        "for text, (source, page) in source_map.items():\n",
        "    chunk_embedding = embedding_model.embed_query(text)  # Get embedding for each chunk\n",
        "    similarity_score = cosine_similarity([response_embedding], [chunk_embedding])[0][0]\n",
        "\n",
        "    if similarity_score >= 0.7:  # Threshold for relevance\n",
        "        relevant_sources.add(f\"{source} (Page {page+1})\")\n",
        "\n",
        "formatted_response = f\"Response: {response_text}\\nSources: {list(relevant_sources) if relevant_sources else ['No sources matched']}\"\n",
        "print(formatted_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxkV6QuupvcL",
        "outputId": "173891fe-bfba-43b7-e4a0-fda57c050ffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: ChÃ o báº¡n, ráº¥t vui Ä‘Æ°á»£c há»— trá»£ báº¡n vá» chÆ°Æ¡ng trÃ¬nh SÄ© Quan ThÃ´ng Tin CSE2021.\n",
            "\n",
            "Theo thÃ´ng tin tá»« buá»•i Info Session CSE2021, Ä‘á»ƒ Ä‘á»§ Ä‘iá»u kiá»‡n tá»‘t nghiá»‡p chÆ°Æ¡ng trÃ¬nh Khoa há»c MÃ¡y tÃ­nh (CSE) ngoÃ i khÃ³a luáº­n tá»‘t nghiá»‡p, sinh viÃªn cáº§n pháº£i Ä‘áº¡t Ä‘Æ°á»£c **150 TC tá»« cÃ¡c há»c pháº§n báº¯t buá»™c** vÃ  **15 TC tá»« thá»±c táº­p chuyÃªn ngÃ nh**.\n",
            "\n",
            "Hy vá»ng thÃ´ng tin nÃ y há»¯u Ã­ch cho báº¡n! Náº¿u cÃ³ báº¥t ká»³ cÃ¢u há»i nÃ o khÃ¡c, Ä‘á»«ng ngáº§n ngáº¡i há»i nhÃ©.\n",
            "Sources: ['/content/CSE_Specific Examination Regulation_Annex A and B_VN.pdf (Page 5)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate Questions and Save to CSV"
      ],
      "metadata": {
        "id": "a6_KAOaqY0Ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import csv\n",
        "import os\n",
        "\n",
        "csv_filename = \"generated_questions.csv\"\n",
        "\n",
        "# Open CSV file for writing with UTF-8 BOM\n",
        "with open(csv_filename, mode=\"a\", newline=\"\", encoding=\"utf-8-sig\") as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Write the header (only \"Question\" column now)\n",
        "    writer.writerow([\"Question\"])\n",
        "\n",
        "    for i, doc in enumerate(pdf_files):\n",
        "        time.sleep(2)  # Avoid hitting API rate limits\n",
        "\n",
        "        # Extract document name (without path)\n",
        "        doc_name = os.path.basename(doc)\n",
        "\n",
        "        # Select relevant chunks\n",
        "        doc_chunks = chunks[i * 3 : (i + 1) * 3]\n",
        "        doc_text = \"\\n\".join([chunk.page_content.strip() for chunk in doc_chunks]).strip()\n",
        "\n",
        "        if not doc_text:\n",
        "            continue  # Skip if the document has no text\n",
        "\n",
        "        # Generate questions with improved prompt\n",
        "        prompt = HumanMessage(content=f\"\"\"Dá»±a vÃ o ná»™i dung tÃ i liá»‡u \"{doc_name}\", hÃ£y táº¡o 5 cÃ¢u há»i Ä‘a dáº¡ng báº±ng tiáº¿ng Viá»‡t.\n",
        "        KhÃ´ng Ä‘Ã¡nh sá»‘ thá»© tá»±, khÃ´ng Ä‘á»ƒ láº¡i khoáº£ng tráº¯ng dÆ° thá»«a, vÃ  má»—i cÃ¢u há»i pháº£i cÃ³ Ä‘á»§ ngá»¯ cáº£nh Ä‘á»ƒ hiá»ƒu Ä‘Æ°á»£c tÃ i liá»‡u liÃªn quan.\"\"\")\n",
        "\n",
        "        response = chat_model.invoke([prompt])\n",
        "        questions = [q.strip() for q in response.content.strip().split(\"\\n\") if q.strip()]  # Clean up questions\n",
        "\n",
        "        # Write each question to the CSV file\n",
        "        for question in questions:\n",
        "            writer.writerow([question])\n",
        "\n",
        "print(f\"âœ… Questions saved to {csv_filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WhJAu7udPkJ",
        "outputId": "bb3ba90c-52ca-4d8b-d095-e86576717653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Questions saved to generated_questions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate Answers"
      ],
      "metadata": {
        "id": "OwAsxS4cZPOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "answer_csv = \"generated_QA.csv\"\n",
        "\n",
        "# Load generated questions\n",
        "df_questions = pd.read_csv(\"generated_questions.csv\")\n",
        "questions = df_questions[\"Question\"].tolist()\n",
        "\n",
        "answers = []\n",
        "answer_sources = []\n",
        "\n",
        "for question in questions:\n",
        "    time.sleep(2)  # Avoid hitting API rate limits\n",
        "\n",
        "    # Retrieve relevant document content\n",
        "    context, _ = augment_prompt(question)\n",
        "    prompt = HumanMessage(content=context)\n",
        "\n",
        "    # Get answer from chatbot model\n",
        "    answer_res = chat_model.invoke([prompt])\n",
        "    answer = answer_res.content.strip()\n",
        "    answers.append(answer)\n",
        "\n",
        "    # Identify relevant sources for the answer\n",
        "    relevant_sources = set()\n",
        "    response_embedding = embedding_model.embed_query(answer)  # Embed answer\n",
        "\n",
        "    for text, (source, page) in source_map.items():\n",
        "        chunk_embedding = embedding_model.embed_query(text)  # Embed document chunk\n",
        "        similarity_score = cosine_similarity([response_embedding], [chunk_embedding])[0][0]\n",
        "\n",
        "        if similarity_score >= 0.7:  # Threshold for relevance\n",
        "            relevant_sources.add(f\"{source} (Page {page+1})\")\n",
        "\n",
        "    # Store relevant sources for answer\n",
        "    source_list = \"; \".join(relevant_sources) if relevant_sources else \"No sources matched\"\n",
        "    answer_sources.append(source_list)\n",
        "\n",
        "# Save answers and sources to CSV\n",
        "df_answers = pd.DataFrame({\"Answer\": answers, \"Relevant Source (Answer)\": answer_sources})\n",
        "df_answers.to_csv(answer_csv, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(f\"âœ… Answers saved to {answer_csv}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccqKpRxNZO-_",
        "outputId": "32a928e8-beff-4654-a4ce-5be1e23fc7db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Answers saved to generated_QA.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install paddlepaddle-gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BQMTJVE2u3s",
        "outputId": "8291ff0f-b63e-417c-a890-9e53c48f7992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting paddlepaddle-gpu\n",
            "  Downloading paddlepaddle_gpu-2.6.2-cp311-cp311-manylinux1_x86_64.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from paddlepaddle-gpu) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.11/dist-packages (from paddlepaddle-gpu) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from paddlepaddle-gpu) (10.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from paddlepaddle-gpu) (4.4.2)\n",
            "Collecting astor (from paddlepaddle-gpu)\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting opt-einsum==3.3.0 (from paddlepaddle-gpu)\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from paddlepaddle-gpu) (4.25.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->paddlepaddle-gpu) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->paddlepaddle-gpu) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->paddlepaddle-gpu) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->paddlepaddle-gpu) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->paddlepaddle-gpu) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->paddlepaddle-gpu) (1.3.1)\n",
            "Downloading paddlepaddle_gpu-2.6.2-cp311-cp311-manylinux1_x86_64.whl (759.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m759.0/759.0 MB\u001b[0m \u001b[31m735.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Installing collected packages: opt-einsum, astor, paddlepaddle-gpu\n",
            "  Attempting uninstall: opt-einsum\n",
            "    Found existing installation: opt_einsum 3.4.0\n",
            "    Uninstalling opt_einsum-3.4.0:\n",
            "      Successfully uninstalled opt_einsum-3.4.0\n",
            "Successfully installed astor-0.8.1 opt-einsum-3.3.0 paddlepaddle-gpu-2.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf pdfplumber camelot-py[cv] paddleocr vietocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KsDwMBvY2Zdu",
        "outputId": "de116dcd-2947-4d04-a38d-bc9518b399d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting paddleocr\n",
            "  Downloading paddleocr-2.10.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting vietocr\n",
            "  Downloading vietocr-0.3.13-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting camelot-py[cv]\n",
            "  Downloading camelot_py-1.0.0-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.1.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "\u001b[33mWARNING: camelot-py 1.0.0 does not provide the extra 'cv'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (8.1.8)\n",
            "Requirement already satisfied: chardet>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (5.2.0)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (1.26.4)\n",
            "Requirement already satisfied: openpyxl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (3.1.5)\n",
            "INFO: pip is looking at multiple versions of camelot-py[cv] to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting camelot-py[cv]\n",
            "  Downloading camelot_py-0.11.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (2.2.2)\n",
            "Collecting pypdf>=3.0.0 (from camelot-py[cv])\n",
            "  Downloading pypdf-5.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (0.9.0)\n",
            "Collecting ghostscript>=0.7 (from camelot-py[cv])\n",
            "  Downloading ghostscript-0.7-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: opencv-python>=3.4.2.17 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (4.11.0.86)\n",
            "Collecting pdftopng>=0.2.3 (from camelot-py[cv])\n",
            "  Downloading pdftopng-0.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.11/dist-packages (from paddleocr) (2.0.7)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from paddleocr) (0.25.2)\n",
            "Collecting pyclipper (from paddleocr)\n",
            "  Downloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting lmdb (from paddleocr)\n",
            "  Downloading lmdb-1.6.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from paddleocr) (4.67.1)\n",
            "Collecting rapidfuzz (from paddleocr)\n",
            "  Downloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from paddleocr) (4.11.0.86)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.11/dist-packages (from paddleocr) (3.0.12)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from paddleocr) (6.0.2)\n",
            "Collecting python-docx (from paddleocr)\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from paddleocr) (4.13.3)\n",
            "Requirement already satisfied: fonttools>=4.24.0 in /usr/local/lib/python3.11/dist-packages (from paddleocr) (4.56.0)\n",
            "Collecting fire>=0.3.0 (from paddleocr)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from paddleocr) (2.32.3)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (from paddleocr) (2.0.5)\n",
            "Requirement already satisfied: albucore in /usr/local/lib/python3.11/dist-packages (from paddleocr) (0.0.23)\n",
            "Collecting einops==0.2.0 (from vietocr)\n",
            "  Downloading einops-0.2.0-py2.py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting gdown==4.4.0 (from vietocr)\n",
            "  Downloading gdown-4.4.0.tar.gz (14 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting prefetch-generator==1.0.1 (from vietocr)\n",
            "  Downloading prefetch_generator-1.0.1.tar.gz (3.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: imgaug==0.4.0 in /usr/local/lib/python3.11/dist-packages (from vietocr) (0.4.0)\n",
            "Collecting albumentations (from paddleocr)\n",
            "  Downloading albumentations-1.4.2-py3-none-any.whl.metadata (36 kB)\n",
            "Collecting Pillow>=9.1 (from pdfplumber)\n",
            "  Downloading pillow-10.2.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations->paddleocr) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from albumentations->paddleocr) (4.12.2)\n",
            "Requirement already satisfied: scikit-learn>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from albumentations->paddleocr) (1.6.1)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from albumentations->paddleocr) (4.11.0.86)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown==4.4.0->vietocr) (3.17.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from gdown==4.4.0->vietocr) (1.17.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from imgaug==0.4.0->vietocr) (3.10.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (from imgaug==0.4.0->vietocr) (2.37.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire>=0.3.0->paddleocr) (2.5.0)\n",
            "Requirement already satisfied: setuptools>=38.6.0 in /usr/local/lib/python3.11/dist-packages (from ghostscript>=0.7->camelot-py[cv]) (75.1.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl>=3.1.0->camelot-py[cv]) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.4->camelot-py[cv]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.4->camelot-py[cv]) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.4->camelot-py[cv]) (2025.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->paddleocr) (3.4.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->paddleocr) (2025.2.18)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image->paddleocr) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->paddleocr) (0.4)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore->paddleocr) (3.12.3)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore->paddleocr) (6.2.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->paddleocr) (2.6)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx->paddleocr) (5.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->paddleocr) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->paddleocr) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->paddleocr) (2025.1.31)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.2->albumentations->paddleocr) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.2->albumentations->paddleocr) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->imgaug==0.4.0->vietocr) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->imgaug==0.4.0->vietocr) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->imgaug==0.4.0->vietocr) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->imgaug==0.4.0->vietocr) (3.2.1)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==4.4.0->vietocr) (1.7.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfplumber-0.11.5-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading paddleocr-2.10.0-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vietocr-0.3.13-py3-none-any.whl (34 kB)\n",
            "Downloading albumentations-1.4.2-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m133.9/133.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading einops-0.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading pillow-10.2.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ghostscript-0.7-py2.py3-none-any.whl (25 kB)\n",
            "Downloading lmdb-1.6.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (297 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m297.8/297.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdftopng-0.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.3.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading camelot_py-0.11.0-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (969 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m969.6/969.6 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: gdown, prefetch-generator, fire\n",
            "  Building wheel for gdown (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-4.4.0-py3-none-any.whl size=14815 sha256=7b887e785bc8af30cc00155f6804bab33ddbeddfcb04a060f3095e86cb4449ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/17/f7/f9585b8ed11a19d05b50f0ab33aa6635a5179241b92982ab27\n",
            "  Building wheel for prefetch-generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for prefetch-generator: filename=prefetch_generator-1.0.1-py3-none-any.whl size=3941 sha256=a00c06173724ffc493f2cf1b9876c1fd44272f422a2295b0fe0afb7f173ee6f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/24/c0/c552730f6b36b3dfca27ebea2b16c34d144574f8373f4a9d45\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=05efe23335f4502907af3c8b52dc9dad137f19a833d93f194d03777c868f6486\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built gdown prefetch-generator fire\n",
            "Installing collected packages: pyclipper, prefetch-generator, lmdb, einops, rapidfuzz, python-docx, pypdfium2, pypdf, pymupdf, Pillow, pdftopng, ghostscript, fire, pdfminer.six, gdown, pdfplumber, camelot-py, albumentations, vietocr, paddleocr\n",
            "  Attempting uninstall: einops\n",
            "    Found existing installation: einops 0.8.1\n",
            "    Uninstalling einops-0.8.1:\n",
            "      Successfully uninstalled einops-0.8.1\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: pillow 11.1.0\n",
            "    Uninstalling pillow-11.1.0:\n",
            "      Successfully uninstalled pillow-11.1.0\n",
            "  Attempting uninstall: pdfminer.six\n",
            "    Found existing installation: pdfminer.six 20240706\n",
            "    Uninstalling pdfminer.six-20240706:\n",
            "      Successfully uninstalled pdfminer.six-20240706\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 5.2.0\n",
            "    Uninstalling gdown-5.2.0:\n",
            "      Successfully uninstalled gdown-5.2.0\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 2.0.5\n",
            "    Uninstalling albumentations-2.0.5:\n",
            "      Successfully uninstalled albumentations-2.0.5\n",
            "Successfully installed Pillow-10.2.0 albumentations-1.4.2 camelot-py-0.11.0 einops-0.2.0 fire-0.7.0 gdown-4.4.0 ghostscript-0.7 lmdb-1.6.2 paddleocr-2.10.0 pdfminer.six-20231228 pdfplumber-0.11.5 pdftopng-0.2.4 prefetch-generator-1.0.1 pyclipper-1.3.0.post6 pymupdf-1.25.3 pypdf-5.3.1 pypdfium2-4.30.1 python-docx-1.1.2 rapidfuzz-3.12.2 vietocr-0.3.13\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "pdfminer"
                ]
              },
              "id": "419a0dc7df0c4cc98bfb6c8b4b708148"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_test_file = '/content/sample_data/20240918_466_QD-DHVD_Ban hanh Quy che dao tao Nam dai cuong cua VGU-VN.pdf'"
      ],
      "metadata": {
        "id": "RwjZNtwx2rGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF for text & images\n",
        "import pdfplumber  # Extract tables\n",
        "import camelot  # Advanced table extraction\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from paddleocr import PaddleOCR\n",
        "from vietocr.tool.predictor import Predictor\n",
        "from vietocr.tool.config import Cfg\n",
        "\n",
        "# Initialize OCR Models\n",
        "paddle_ocr = PaddleOCR(use_angle_cls=True, lang=\"vi\")\n",
        "\n",
        "config = Cfg.load_config_from_name(\"vgg_transformer\")\n",
        "config['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "vietocr_model = Predictor(config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IlUNH_j2dIQ",
        "outputId": "85b1934c-084f-4b74-e95c-905a211cb7e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025/03/14 08:57:39] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, use_gcu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/root/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/root/.paddleocr/whl/rec/latin/latin_PP-OCRv3_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/usr/local/lib/python3.11/dist-packages/paddleocr/ppocr/utils/dict/latin_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='/root/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, onnx_providers=False, onnx_sess_options=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, formula_algorithm='LaTeXOCR', formula_model_dir=None, formula_char_dict_path=None, formula_batch_num=1, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, formula=False, ocr=True, recovery=False, recovery_to_markdown=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='vi', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19_bn-c79401a0.pth\" to /root/.cache/torch/hub/checkpoints/vgg19_bn-c79401a0.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 548M/548M [00:04<00:00, 125MB/s]\n",
            "18533it [00:09, 1900.49it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    full_text = []\n",
        "    for page in doc:\n",
        "        full_text.append(page.get_text(\"text\"))\n",
        "    return \"\\n\".join(full_text)\n",
        "\n",
        "# Function to extract tables using PDFPlumber\n",
        "def extract_tables(pdf_path):\n",
        "    tables = []\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            tables.extend(page.extract_tables())\n",
        "    return tables\n",
        "\n",
        "# Function to extract images from PDF\n",
        "def extract_images(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    images = []\n",
        "    for i, page in enumerate(doc):\n",
        "        for img_index, img in enumerate(page.get_images(full=True)):\n",
        "            xref = img[0]\n",
        "            base_image = doc.extract_image(xref)\n",
        "            image_data = base_image[\"image\"]\n",
        "            image = Image.open(io.BytesIO(image_data))\n",
        "            images.append(image)\n",
        "    return images\n",
        "\n",
        "# Function to apply PaddleOCR on extracted images\n",
        "def ocr_paddle(image):\n",
        "    image_np = np.array(image)\n",
        "    results = paddle_ocr.ocr(image_np, cls=True)\n",
        "    return \" \".join([res[1][0] for line in results for res in line if len(res) > 1])\n",
        "\n",
        "# Function to apply VietOCR on extracted images\n",
        "def ocr_vietocr(image):\n",
        "    return vietocr_model.predict(image)\n",
        "\n",
        "# Main function to process PDF\n",
        "def process_pdf(pdf_path):\n",
        "    print(\"Extracting text...\")\n",
        "    extracted_text = extract_text(pdf_path)\n",
        "\n",
        "    print(\"Extracting tables...\")\n",
        "    extracted_tables = extract_tables(pdf_path)\n",
        "\n",
        "    print(\"Extracting images...\")\n",
        "    extracted_images = extract_images(pdf_path)\n",
        "\n",
        "    print(\"Running OCR on images...\")\n",
        "    ocr_results = [ocr_vietocr(img) for img in extracted_images]\n",
        "\n",
        "    # Combine results\n",
        "    return {\n",
        "        \"text\": extracted_text,\n",
        "        \"tables\": extracted_tables,\n",
        "        \"ocr_results\": ocr_results\n",
        "    }\n",
        "\n",
        "# Run the pipeline on a sample PDF\n",
        "pdf_file = \"/content/sample_data/your_file.pdf\"\n",
        "results = process_pdf(pdf_file)\n",
        "\n",
        "# Print results\n",
        "print(\"Extracted Text:\\n\", results[\"text\"])\n",
        "print(\"\\nExtracted Tables:\\n\", results[\"tables\"])\n",
        "print(\"\\nOCR Results from Images:\\n\", results[\"ocr_results\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "5ELLFCDI2XNe",
        "outputId": "23ad4ce3-5f3c-4228-915a-f6c2913b2f19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting text...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "no such file: '/content/sample_data/your_file.pdf'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-08b77f915962>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m# Run the pipeline on a sample PDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mpdf_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/sample_data/your_file.pdf\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Print results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-08b77f915962>\u001b[0m in \u001b[0;36mprocess_pdf\u001b[0;34m(pdf_path)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extracting text...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mextracted_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extracting tables...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-08b77f915962>\u001b[0m in \u001b[0;36mextract_text\u001b[0;34m(pdf_path)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Function to extract text from PDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfitz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mfull_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pymupdf/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, stream, filetype, rect, width, height, fontsize)\u001b[0m\n\u001b[1;32m   2916\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2917\u001b[0m                     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"no such file: '{filename}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2918\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2919\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2920\u001b[0m                     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"'{filename}' is no file\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: no such file: '/content/sample_data/your_file.pdf'"
          ]
        }
      ]
    }
  ]
}